{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1abb4b457f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01050741  0.00953022  0.01899089]\n",
      "[0 1] [ 0.99494904  0.99303913  0.98110983]\n",
      "[1 0] [ 0.99493338  0.99301834  0.98106542]\n",
      "[1 1] [ 0.00703042  0.00715337  0.02135516]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "  \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        myList=[]\n",
    "        avList=[]\n",
    "       \n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "      \n",
    "           \n",
    "            error = y[i] - a[-1]\n",
    "            myList.append(np.sum(error**2))# mean squard error MSE  \n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            \n",
    "                \n",
    "          \n",
    "\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "           \n",
    "            cnt = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                cnt =cnt + layer.T.dot(delta)\n",
    "                self.weights[i] =self.weights[i]+ learning_rate * cnt# save in smthing momentum\n",
    "            t = np.average(myList)\n",
    "            avList.append(t) \n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                t = np.average(myList)\n",
    "                avList.append(t)\n",
    "           \n",
    "           # plt.show()\n",
    "        #print(myList)\n",
    "        \n",
    "        #plt.plot(myList[1])\n",
    "        plt.plot(avList)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-987201ca294f>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-987201ca294f>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    print 'Epoch {0}: {1} / {2}',format(\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "      \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "       \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "      \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print 'Epoch {0}: {1} / {2}',format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print 'Epoch {0} complete',format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "       \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "      \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "   \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "   \n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-8-b452286f0f11>, line 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-b452286f0f11>\"\u001b[1;36m, line \u001b[1;32m142\u001b[0m\n\u001b[1;33m    print \"Epoch %s training complete\" % j\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
